# Whatâ€™s New in OPEA v0.8
- Broaden functionality coverage
    - OPEA v0.8 Support LVM(Large Vision Models), it provides a microservice for llava on Intel Gaudi2. It also introduces  an initial Agent microservice, enabling OPEA to process information, evaluate situations, and take actionable steps. Additional components, such as vLLM on Ray, offering users more choices for different AI functionalities. FAQGen (Frequently Asked Questions Generation) is a new example of an LLM application.

- Deploy across more platforms
    - We have validated ChatQnA on AWS, including the GenAI Microservices Connector (GMC), Kubernetes manifests, and Docker. It can also operate on AIPC. Kubernetes manifests for ChatQnA, DocSum, CodeGen, and CodeTrans are created. The document for deploying OPEA with Docker Hub is included in this release. GMC supports the launching, monitoring, and updating of GenAI microservice applications on Kubernetes.

- Added value
    - OPEA v0.8 creates retrieval metrics, RAG accuracy and performance benchmarks, helping users choose the suitable components.
