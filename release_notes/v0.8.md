# Whatâ€™s New in OPEA v0.8

- Broaden functionality
    - Supported frequently asked questions (FAQs) generation GenAI example
    - Expanded the support of LLMs such as Llama3.1 and Qwen2 and supported LVMs such as llava
    - Enabled end-to-end peformance and accuracy benchmarking
    - Supported the experimental Agent microservice and GenAI example
    - Supported LLM serving on Ray

- Multi-platform support
    - Released the docker images of GenAI components under OPEA dockerhub and supported the deployment with docker
    - Supported the cloud native deployment through Kubernetes manifests and GenAI Microservices Connector (GMC)
    - Enabled the experimental authentication and authorization support using JWT tokens
    - Validated ChatQnA on multiple platforms such as Xeon, Gaudi, AIPC and Nvidia, and AWS

Details: [commits_doc](v0.8_details.md)
OPEA Docker Hub: https://hub.docker.com/u/opea
