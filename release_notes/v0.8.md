# Whatâ€™s New in OPEA v0.8

- Broaden functionality
    - Supports frequently asked questions (FAQs) generation GenAI example
    - Expands the supports of LLMs such as Llama3.1 and Qwen2 and supported LVMs such as llava
    - Enables end-to-end performance and accuracy benchmarking
    - Supports the experimental Agent microservice and GenAI example
    - Supports LLM serving on Ray

- Multi-platform support
    - Releases the Docker images of GenAI components under OPEA dockerhub and supports the deployment with Docker
    - Supports cloud-native deployment through Kubernetes manifests and GenAI Microservices Connector (GMC)
    - Enables the experimental authentication and authorization support using JWT tokens
    - Validates ChatQnA on multiple platforms such as Xeon, Gaudi, AIPC, Nvidia, and AWS

Details: [commits_doc](v0.8_details.md)
OPEA Docker Hub: https://hub.docker.com/u/opea
