# Whatâ€™s New in OPEA v0.8
- Broaden functionality coverage
    - OPEA v0.8 Support LVM(Large Vision Models), it provides a microservice for llava on Intel Gaudi2. It also create an initial Agent microservice, OPEA now has the capability to process information, evaluate situations, and take actionable steps. Meanwhile more components are coming, such as vLLM on Ray, giving users more choices for different AI functionalities. FAQGen (Frequently Asked Questions Generation) is a new example of LLM application.
- Deploy across more platforms
    - We validate ChatQnA on AWS, including GenAI Microservices Connector (GMC), Kubernetes manifests and docker. Meanwhile it can work on AIPC. Kubernetes manifests for ChatQnA, DocSum, CodeGen and CodeTrans are created. The document for deploying OPEA with Docker Hub is also included in this release.GMC supports the launching, monitoring, and updating of GenAI microservice Applications on Kubernetes. GMC essentially supports a Kubernetes Custom Resource Definition for GenAI chains/pipelines that may be comprised of sequential, conditional, and parallel steps. Kubernetes manifests for ChatQnA, DocSum, CodeGen and CodeTrans are created. Deployment of deploy with dockerhub is ready.
- Added value
    - OPEA v0.8 creates retrieval metrics, RAG accuracy and performance benchmarks, helping users choose the suitable components.
